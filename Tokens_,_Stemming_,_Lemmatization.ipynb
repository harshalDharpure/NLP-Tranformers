{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOrS9dbhMfx8haHnfe1WhZy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshalDharpure/NLP-Tranformers/blob/main/Tokens_%2C_Stemming_%2C_Lemmatization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLyY5GwG_TFZ"
      },
      "outputs": [],
      "source": [
        "tweet = \"\"\"Iâ€™m amazed how often in practice, not only does a @huggingface NLP model solve your problem, but one of their public finetuned checkpoints, is good enough for the job.\n",
        "\n",
        "Both impressed, and a little disappointed how rarely I get to actually train a model that matters :(\"\"\"\n",
        "\n",
        "tweet.split()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  [char for char in tweet][:10]"
      ],
      "metadata": {
        "id": "sxXM2eVv_eEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Stemming is performend on the text\n",
        "words_to_stem = ['happy', 'happiest', 'happier', 'cactus', 'cactii', 'elephant', 'elephants', 'amazed', 'amazing', 'amazingly', 'cement', 'owed', 'maximum']\n",
        "\n",
        "from nltk.stem import PorterStemmer,LancasterStemmer\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "stemmed = [(porter.stem(word),lancaster.stem(word)) for word in words_to_stem ]\n",
        "print(\"Porter | Lancaster\")\n",
        "for stem in stemmed:\n",
        "  print(f\"{stem[0]} |{stem[1]}\")"
      ],
      "metadata": {
        "id": "MSn9-iOF_oU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Lemmatization\n",
        "\n",
        "words = ['amaze', 'amazed', 'amazing']"
      ],
      "metadata": {
        "id": "mX6yw1-vCBFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install WordNet"
      ],
      "metadata": {
        "id": "GIQyHKYVCj5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "[lemmatizer.lemmatize(word) for word in words]"
      ],
      "metadata": {
        "id": "3IOHkds1Cpsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "[lemmatizer.lemmatize(word,wordnet.VERB) for word in words]"
      ],
      "metadata": {
        "id": "_yc1clY1DKnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hRNoqciLDcRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YABnNL0X_lvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d7d35kSN_kOV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}